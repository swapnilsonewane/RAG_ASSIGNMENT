{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fd64b3",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc4dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swapnilsonewane/Documents/Viral_labs_assessment/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import random\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from sentence_transformers.util import cos_sim\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "OUTPUT_DIR = \"./trained_model/\"\n",
    "OUTPUT_RESULTS_DIR = \"./results/\"\n",
    "TRAINING_DATA_DIR = \"./train_data/\"\n",
    "\n",
    "for path in [ OUTPUT_DIR, OUTPUT_RESULTS_DIR, TRAINING_DATA_DIR]:\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b76a1a",
   "metadata": {},
   "source": [
    "### STEP 1: PDF PROCESSING & CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f2a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract text from a single PDF\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\\n\"\n",
    "        \n",
    "        text = text.replace('\\x00', '')\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Error reading {pdf_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def chunk_text_semantic(text: str, \n",
    "                        max_tokens: int = 256,\n",
    "                        overlap_tokens: int = 50,\n",
    "                        min_tokens: int = 30) -> List[str]:\n",
    "    \"\"\"\n",
    "    Semantic chunking using sentence boundaries.\n",
    "    More suitable for embedding models than character-based splitting.\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        sentence_tokens = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        \n",
    "        if sentence_tokens > max_tokens:\n",
    "            words = sentence.split()\n",
    "            temp_chunk = []\n",
    "            temp_tokens = 0\n",
    "            \n",
    "            for word in words:\n",
    "                word_tokens = len(tokenizer.encode(word, add_special_tokens=False))\n",
    "                if temp_tokens + word_tokens <= max_tokens - 10:\n",
    "                    temp_chunk.append(word)\n",
    "                    temp_tokens += word_tokens\n",
    "                else:\n",
    "                    if temp_chunk:\n",
    "                        chunks.append(' '.join(temp_chunk))\n",
    "                    temp_chunk = [word]\n",
    "                    temp_tokens = word_tokens\n",
    "            \n",
    "            if temp_chunk:\n",
    "                chunks.append(' '.join(temp_chunk))\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "\n",
    "            if overlap_tokens > 0 and current_chunk:\n",
    "                overlap_text = ' '.join(current_chunk)\n",
    "                overlap_tok = tokenizer.encode(overlap_text, add_special_tokens=False)[-overlap_tokens:]\n",
    "                overlap_text = tokenizer.decode(overlap_tok)\n",
    "                current_chunk = [overlap_text, sentence]\n",
    "                current_tokens = len(tokenizer.encode(' '.join(current_chunk), add_special_tokens=False))\n",
    "            else:\n",
    "                current_chunk = [sentence]\n",
    "                current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    chunks = [c for c in chunks if len(tokenizer.encode(c, add_special_tokens=False)) >= min_tokens]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder: str,\n",
    "                           max_tokens: int = 256,\n",
    "                           overlap_tokens: int = 50) -> List[Dict]:\n",
    "    \"\"\"Extract and chunk text from all PDFs in folder\"\"\"\n",
    "    pdf_path = Path(pdf_folder)\n",
    "    if not pdf_path.exists():\n",
    "        raise ValueError(f\"PDF folder not found: {pdf_folder}\")\n",
    "    \n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f\"No PDF files found in {pdf_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        text_chunks = chunk_text_semantic(text, max_tokens, overlap_tokens)\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            all_chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'source': pdf_file.name,\n",
    "                'chunk_id': f\"{pdf_file.stem}_chunk_{i}\",\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(text_chunks)\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úì Extracted {len(all_chunks)} chunks from {len(pdf_files)} PDFs\")\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2c91f",
   "metadata": {},
   "source": [
    "### Training Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76734b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "_nlp = None\n",
    "_sentence_model = None\n",
    "\n",
    "def get_nlp():\n",
    "    global _nlp\n",
    "    if _nlp is None:\n",
    "        import spacy\n",
    "        try:\n",
    "            _nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            import subprocess\n",
    "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            _nlp = spacy.load(\"en_core_web_sm\")\n",
    "    return _nlp\n",
    "\n",
    "\n",
    "class QueryGenerator:\n",
    "    \"\"\"\n",
    "    Data-driven query generator that learns patterns from your corpus.\n",
    "    No hardcoded rules - adapts to domain vocabulary and structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab_stats = None\n",
    "        self.phrase_patterns = None\n",
    "        self.question_templates = None\n",
    "        \n",
    "    def learn_from_corpus(self, chunks: List[Dict], sample_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Learn vocabulary, patterns, and structure from the corpus itself.\n",
    "        This replaces all hardcoded rules with data-driven patterns.\n",
    "        \"\"\"\n",
    "        print(\"Learning corpus patterns...\")\n",
    "\n",
    "        sampled = random.sample(chunks, min(sample_size, len(chunks)))\n",
    "        \n",
    "        nlp = get_nlp()\n",
    "\n",
    "        all_noun_phrases = []\n",
    "        all_entities = []\n",
    "        all_verbs = []\n",
    "        sentence_starts = []\n",
    "        bigrams = []\n",
    "        trigrams = []\n",
    "        \n",
    "        for chunk in sampled:\n",
    "            doc = nlp(chunk['text'][:500])\n",
    "\n",
    "            for chunk_item in doc.noun_chunks:\n",
    "                text = self._clean_phrase(chunk_item.text)\n",
    "                if text:\n",
    "                    all_noun_phrases.append(text.lower())\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                text = self._clean_phrase(ent.text)\n",
    "                if text and len(text.split()) <= 4:\n",
    "                    all_entities.append((text.lower(), ent.label_))\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.pos_ == 'VERB' and not token.is_stop:\n",
    "                    all_verbs.append(token.lemma_.lower())\n",
    "\n",
    "            sentences = [s.text.strip() for s in doc.sents if len(s.text.strip()) > 20]\n",
    "            for sent in sentences[:3]:\n",
    "                words = sent.split()[:5]\n",
    "                sentence_starts.append(' '.join(words).lower())\n",
    "\n",
    "            tokens = [t.text.lower() for t in doc if not t.is_stop and t.is_alpha]\n",
    "            if len(tokens) >= 2:\n",
    "                bigrams.extend([f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)])\n",
    "            if len(tokens) >= 3:\n",
    "                trigrams.extend([f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\" for i in range(len(tokens)-2)])\n",
    "\n",
    "        self.vocab_stats = {\n",
    "            'top_noun_phrases': [item for item, count in Counter(all_noun_phrases).most_common(200)],\n",
    "            'top_entities': [item[0] for item, count in Counter(all_entities).most_common(100)],\n",
    "            'entity_types': Counter([item[1] for item in all_entities]),\n",
    "            'top_verbs': [item for item, count in Counter(all_verbs).most_common(50)],\n",
    "            'common_bigrams': [item for item, count in Counter(bigrams).most_common(100)],\n",
    "            'common_trigrams': [item for item, count in Counter(trigrams).most_common(100)],\n",
    "            'sentence_patterns': Counter(sentence_starts).most_common(50)\n",
    "        }\n",
    "\n",
    "        self.question_templates = self._generate_templates()\n",
    "        \n",
    "        print(f\"‚úì Learned {len(self.vocab_stats['top_noun_phrases'])} key phrases\")\n",
    "        print(f\"‚úì Learned {len(self.vocab_stats['top_entities'])} entities\")\n",
    "        print(f\"‚úì Generated {len(self.question_templates)} query templates\")\n",
    "        \n",
    "    def _clean_phrase(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize phrases\"\"\"\n",
    "        text = text.strip()\n",
    "\n",
    "        text = re.sub(r'^(the|a|an|this|that|these|those)\\s+', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text if len(text) > 3 else ''\n",
    "    \n",
    "    def _generate_templates(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate query templates dynamically based on common linguistic patterns.\n",
    "        These adapt to your domain's vocabulary and structure.\n",
    "        \"\"\"\n",
    "        templates = []\n",
    "\n",
    "        question_words = ['what', 'how', 'why', 'when', 'where', 'which', 'who']\n",
    "        verbs = self.vocab_stats['top_verbs'][:20] if self.vocab_stats['top_verbs'] else ['is', 'are', 'does']\n",
    "        \n",
    "        for qw in question_words:\n",
    "            for verb in verbs[:5]:\n",
    "                templates.append({\n",
    "                    'pattern': f\"{qw} {verb} {{concept}}\",\n",
    "                    'type': 'question'\n",
    "                })\n",
    "\n",
    "        phrase_starters = ['explain', 'describe', 'define', 'overview of', 'details about', \n",
    "                          'information on', 'summary of', 'analysis of']\n",
    "        for starter in phrase_starters:\n",
    "            templates.append({\n",
    "                'pattern': f\"{starter} {{concept}}\",\n",
    "                'type': 'imperative'\n",
    "            })\n",
    "\n",
    "        templates.extend([\n",
    "            {'pattern': '{concept}', 'type': 'keyword'},\n",
    "            {'pattern': '{concept} {concept2}', 'type': 'multi_keyword'},\n",
    "            {'pattern': '{concept} overview', 'type': 'keyword_suffix'},\n",
    "            {'pattern': '{concept} definition', 'type': 'keyword_suffix'},\n",
    "        ])\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def _extract_key_phrases(self, text: str, max_phrases: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract most relevant phrases using TF-IDF-like scoring against learned vocabulary.\n",
    "        \"\"\"\n",
    "        nlp = get_nlp()\n",
    "        doc = nlp(text[:800])\n",
    "        \n",
    "        phrases = []\n",
    "        scores = []\n",
    "\n",
    "        for chunk in doc.noun_chunks:\n",
    "            phrase = self._clean_phrase(chunk.text).lower()\n",
    "            if not phrase:\n",
    "                continue\n",
    "            \n",
    "            score = 0\n",
    "\n",
    "            if phrase in self.vocab_stats['top_noun_phrases'][:50]:\n",
    "                score += 10\n",
    "            elif phrase in self.vocab_stats['top_noun_phrases']:\n",
    "                score += 5\n",
    "\n",
    "            word_count = len(phrase.split())\n",
    "            if 2 <= word_count <= 4:\n",
    "                score += word_count * 2\n",
    "\n",
    "            if phrase in self.vocab_stats['common_bigrams']:\n",
    "                score += 3\n",
    "            if phrase in self.vocab_stats['common_trigrams']:\n",
    "                score += 5\n",
    "            \n",
    "            phrases.append(phrase)\n",
    "            scores.append(score)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            phrase = self._clean_phrase(ent.text).lower()\n",
    "            if phrase and len(phrase.split()) <= 4:\n",
    "                if phrase in self.vocab_stats['top_entities'][:30]:\n",
    "                    phrases.append(phrase)\n",
    "                    scores.append(8)\n",
    "\n",
    "        if not phrases:\n",
    "            return []\n",
    "        \n",
    "        sorted_phrases = [p for _, p in sorted(zip(scores, phrases), reverse=True)]\n",
    "        return list(dict.fromkeys(sorted_phrases))[:max_phrases]\n",
    "    \n",
    "    def _sentence_to_query(self, sentence: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert informative sentences to queries using learned patterns.\n",
    "        \"\"\"\n",
    "        queries = []\n",
    "        nlp = get_nlp()\n",
    "        doc = nlp(sentence[:200])\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
    "                \n",
    "                subjects = [child.text for child in token.children if child.dep_ in ['nsubj', 'nsubjpass']]\n",
    "                if subjects:\n",
    "                    subject = subjects[0]\n",
    "                    \n",
    "                    if token.lemma_ in ['be', 'have']:\n",
    "                        queries.append(f\"What {token.lemma_} {subject}?\")\n",
    "                    else:\n",
    "                        queries.append(f\"How does {subject} {token.lemma_}?\")\n",
    "        \n",
    "        return queries\n",
    "    \n",
    "    def generate_queries(self, chunk_text: str, num_queries: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate diverse queries for a chunk using learned patterns.\n",
    "        \"\"\"\n",
    "        if not self.vocab_stats:\n",
    "            raise ValueError(\"Must call learn_from_corpus() first!\")\n",
    "        \n",
    "        queries = set()\n",
    "        \n",
    "        key_phrases = self._extract_key_phrases(chunk_text, max_phrases=8)\n",
    "        \n",
    "        if not key_phrases:\n",
    "            return []\n",
    "        \n",
    "        for template in self.question_templates:\n",
    "            if len(queries) >= num_queries * 3:\n",
    "                break\n",
    "            \n",
    "            pattern = template['pattern']\n",
    "            \n",
    "            if '{concept}' in pattern:\n",
    "                for phrase in key_phrases[:4]:\n",
    "                    if '{concept2}' in pattern and len(key_phrases) >= 2:\n",
    "\n",
    "                        for phrase2 in key_phrases[:4]:\n",
    "                            if phrase != phrase2:\n",
    "                                query = pattern.format(concept=phrase, concept2=phrase2)\n",
    "                                queries.add(query)\n",
    "                    else:\n",
    "\n",
    "                        query = pattern.format(concept=phrase)\n",
    "                        queries.add(query)\n",
    "\n",
    "        sentences = [s.strip() for s in re.split(r'[.!?]+', chunk_text) if 30 < len(s.strip()) < 150]\n",
    "        for sent in sentences[:2]:\n",
    "            sent_queries = self._sentence_to_query(sent)\n",
    "            queries.update(sent_queries)\n",
    "\n",
    "        for phrase in key_phrases[:3]:\n",
    "            queries.add(phrase)\n",
    "\n",
    "        clean_queries = []\n",
    "        for q in queries:\n",
    "            q = re.sub(r'\\s+', ' ', q.strip())\n",
    "            if (8 < len(q) < 150 and \n",
    "                len(q.split()) >= 2 and\n",
    "                not q.startswith('the ')):\n",
    "                clean_queries.append(q)\n",
    "\n",
    "        random.shuffle(clean_queries)\n",
    "        return clean_queries[:num_queries]\n",
    "\n",
    "\n",
    "def create_training_dataset(chunks: List[Dict], \n",
    "                           queries_per_chunk: int = 5,\n",
    "                           learn_sample_size: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate training dataset using data-driven approach.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks with 'text' and 'chunk_id'\n",
    "        queries_per_chunk: Number of queries to generate per chunk\n",
    "        learn_sample_size: Number of chunks to use for learning patterns\n",
    "    \n",
    "    Returns:\n",
    "        List of {'query', 'positive', 'chunk_id'} training pairs\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    generator = QueryGenerator()\n",
    "    generator.learn_from_corpus(chunks, sample_size=learn_sample_size)\n",
    "\n",
    "    training_data = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Generating queries\"):\n",
    "        try:\n",
    "            queries = generator.generate_queries(chunk['text'], queries_per_chunk)\n",
    "            \n",
    "            if not queries:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            for query in queries:\n",
    "                training_data.append({\n",
    "                    'query': query,\n",
    "                    'positive': chunk['text'],\n",
    "                    'chunk_id': chunk['chunk_id']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n Generated {len(training_data)} query-chunk pairs\")\n",
    "    if skipped > 0:\n",
    "        print(f\"Skipped {skipped} chunks\")\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5357529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOMAIN-SPECIFIC EMBEDDING TRAINING PIPELINE\n",
      "============================================================\n",
      "\n",
      "STEP 1: Extracting and chunking PDFs...\n",
      "Found 5 PDF files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Extracted 407 chunks from 5 PDFs\n",
      "‚úì Saved 407 chunks\n",
      "\n",
      " STEP 2: Generating synthetic queries...\n",
      "Learning corpus patterns...\n",
      "‚úì Learned 200 key phrases\n",
      "‚úì Learned 100 entities\n",
      "‚úì Generated 47 query templates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 407/407 [00:17<00:00, 23.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generated 2030 query-chunk pairs\n",
      "Skipped 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Training_data_and_chunking():\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "\n",
    "    CONFIG = {\n",
    "        'pdf_folder': DATA_DIR,\n",
    "        'output_dir': TRAINING_DATA_DIR,\n",
    "        'base_model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'queries_per_chunk': 5,\n",
    "    }\n",
    "\n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DOMAIN-SPECIFIC EMBEDDING TRAINING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nSTEP 1: Extracting and chunking PDFs...\")\n",
    "    chunks = extract_text_from_pdfs(CONFIG['pdf_folder'])\n",
    "    \n",
    "    # Save chunks\n",
    "    with open(output_dir / \"chunks.json\", 'w') as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "    print(f\"‚úì Saved {len(chunks)} chunks\")\n",
    "\n",
    "    print(\"\\n STEP 2: Generating synthetic queries...\")\n",
    "\n",
    "    \n",
    "    training_data = create_training_dataset(\n",
    "        chunks,\n",
    "        queries_per_chunk=CONFIG['queries_per_chunk'],\n",
    "        learn_sample_size=min(1000, len(chunks))\n",
    "    )\n",
    "\n",
    "    with open(output_dir / \"training_data_raw.json\", 'w') as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "\n",
    "    return training_data, chunks\n",
    "\n",
    "training_data, chunks = Training_data_and_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6fbcd5",
   "metadata": {},
   "source": [
    "### STEP 2: HARD NEGATIVE MINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33c940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_hard_negatives_efficient(\n",
    "    training_data: List[Dict], \n",
    "    chunks: List[Dict], \n",
    "    base_model: SentenceTransformer, \n",
    "    num_negatives: int = 3,\n",
    "    use_cross_encoder: bool = True,\n",
    "    cross_encoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ") -> List[Dict]:\n",
    "\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    \n",
    "    cross_encoder = None\n",
    "    if use_cross_encoder:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        print(\"   Loading cross-encoder for reranking...\")\n",
    "        cross_encoder = CrossEncoder(cross_encoder_model)\n",
    "    \n",
    "    # Prepare BM25 index\n",
    "    corpus = [c['text'] for c in chunks]\n",
    "    tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # Embed all chunks once with bi-encoder\n",
    "    print(\"   Encoding chunks with bi-encoder...\")\n",
    "    chunk_embeddings = base_model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "    \n",
    "    enriched_data = []\n",
    "    \n",
    "    for item in tqdm(training_data, desc=\"Mining hard negatives\"):\n",
    "        query = item['query']\n",
    "        positive_id = item['chunk_id']\n",
    "        positive_text = item['positive']\n",
    "        \n",
    "        # ========== STAGE 1: Fast Candidate Retrieval ==========\n",
    "        # BM25 retrieval (lexical)\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = bm25.get_scores(tokenized_query)\n",
    "        top_bm25_indices = np.argsort(bm25_scores)[-30:][::-1]  # Top 30\n",
    "        \n",
    "        # Semantic retrieval (bi-encoder)\n",
    "        query_emb = base_model.encode(query, convert_to_tensor=True)\n",
    "        semantic_scores = cos_sim(query_emb, chunk_embeddings)[0]\n",
    "        top_sem_indices = torch.argsort(semantic_scores, descending=True)[:30].tolist()\n",
    "        \n",
    "        # Combine candidates and filter out the positive\n",
    "        candidate_indices = list(set(top_bm25_indices.tolist()) | set(top_sem_indices))\n",
    "        candidate_indices = [i for i in candidate_indices if chunks[i]['chunk_id'] != positive_id]\n",
    "        \n",
    "        if not candidate_indices:\n",
    "            continue\n",
    "\n",
    "        if cross_encoder is not None and len(candidate_indices) > 0:\n",
    "\n",
    "            pairs = [[query, corpus[idx]] for idx in candidate_indices]\n",
    "        \n",
    "            ce_scores = cross_encoder.predict(pairs)\n",
    "        \n",
    "            bi_scores = semantic_scores[candidate_indices].cpu().numpy()\n",
    "            \n",
    "            bi_normalized = (bi_scores - bi_scores.min()) / (bi_scores.max() - bi_scores.min() + 1e-8)\n",
    "            ce_normalized = (ce_scores - ce_scores.min()) / (ce_scores.max() - ce_scores.min() + 1e-8)\n",
    "\n",
    "            hardness_scores = bi_normalized - ce_normalized\n",
    "\n",
    "            hard_negative_indices = []\n",
    "            sorted_indices = np.argsort(hardness_scores)[::-1]\n",
    "            \n",
    "            for idx in sorted_indices:\n",
    "                candidate_idx = candidate_indices[idx]\n",
    "\n",
    "                if ce_scores[idx] < 0.5 and len(hard_negative_indices) < num_negatives:\n",
    "                    hard_negative_indices.append(candidate_idx)\n",
    "\n",
    "            if len(hard_negative_indices) < num_negatives:\n",
    "                remaining = num_negatives - len(hard_negative_indices)\n",
    "                for idx in sorted_indices:\n",
    "                    candidate_idx = candidate_indices[idx]\n",
    "                    if candidate_idx not in hard_negative_indices:\n",
    "                        hard_negative_indices.append(candidate_idx)\n",
    "                        if len(hard_negative_indices) >= num_negatives:\n",
    "                            break\n",
    "            \n",
    "            negatives = [corpus[idx] for idx in hard_negative_indices[:num_negatives]]\n",
    "        \n",
    "        else:\n",
    "            sorted_by_similarity = sorted(\n",
    "                candidate_indices, \n",
    "                key=lambda i: semantic_scores[i].item(), \n",
    "                reverse=True\n",
    "            )\n",
    "            negatives = [corpus[idx] for idx in sorted_by_similarity[:num_negatives]]\n",
    "\n",
    "        if negatives:\n",
    "            enriched_data.append({\n",
    "                'query': query,\n",
    "                'positive': positive_text,\n",
    "                'negatives': negatives,\n",
    "                'chunk_id': positive_id\n",
    "            })\n",
    "    \n",
    "    print(f\"   ‚úì Generated {len(enriched_data)} training examples with hard negatives\")\n",
    "    return enriched_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f35b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STEP 3: Mining hard negatives...\n",
      "   Loading cross-encoder for reranking...\n",
      "   Encoding chunks with bi-encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  5.82it/s]\n",
      "Mining hard negatives: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2030/2030 [21:23<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Generated 2030 training examples with hard negatives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Negative_samples(training_data: List[Dict], chunks: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "\n",
    "    CONFIG = {\n",
    "        'pdf_folder': DATA_DIR,\n",
    "        'output_dir': TRAINING_DATA_DIR,\n",
    "        'base_model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'queries_per_chunk': 5,\n",
    "        'num_hard_negatives': 3,\n",
    "    }\n",
    "\n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n STEP 3: Mining hard negatives...\")\n",
    "    base_model = SentenceTransformer(CONFIG['base_model'])\n",
    "    \n",
    "    training_data_enriched = mine_hard_negatives_efficient(\n",
    "        training_data,\n",
    "        chunks,\n",
    "        base_model,\n",
    "        num_negatives=CONFIG['num_hard_negatives'],\n",
    "    )\n",
    "\n",
    "    with open(output_dir / \"training_data_enriched.json\", 'w') as f:\n",
    "        json.dump(training_data_enriched, f, indent=2)\n",
    "    return training_data_enriched\n",
    "\n",
    "training_data_enriched =  Negative_samples(training_data, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af301ad",
   "metadata": {},
   "source": [
    "### STEP 3: MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Tuple\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def train_embedding_model(\n",
    "    train_data: List[Dict],\n",
    "    base_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    output_path: str = \"./custom_embedding_model\",\n",
    "    val_data: Optional[List[Dict]] = None,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    warmup_ratio: float = 0.1,\n",
    "    use_amp: bool = True\n",
    ") -> Tuple[SentenceTransformer, Dict]:\n",
    "    \"\"\"\n",
    "    Train custom embedding model with hard negatives.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        training_stats: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    print(f\"Loading base model: {base_model_name}\")\n",
    "    model = SentenceTransformer(base_model_name)\n",
    "\n",
    "    print(\"Preparing training data...\")\n",
    "    train_examples = []\n",
    "    \n",
    "    for item in train_data:\n",
    "        query = item['query']\n",
    "        positive = item['positive']\n",
    "        negatives = item.get('negatives', [])\n",
    "    \n",
    "        if negatives:\n",
    "            for neg in negatives:\n",
    "                train_examples.append(InputExample(texts=[query, positive, neg]))\n",
    "        else:\n",
    "            train_examples.append(InputExample(texts=[query, positive]))\n",
    "    \n",
    "    print(f\"Created {len(train_examples)} training examples\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples, \n",
    "        shuffle=True, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if any('negatives' in item for item in train_data):\n",
    "        print(\"Using MultipleNegativesRankingLoss (with hard negatives)\")\n",
    "        train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    else:\n",
    "        print(\"Using ContrastiveLoss\")\n",
    "        train_loss = losses.ContrastiveLoss(model)\n",
    "\n",
    "    evaluator = None\n",
    "    if val_data:\n",
    "        print(\"Setting up validation evaluators...\")\n",
    "\n",
    "        val_examples = []\n",
    "        for item in val_data:\n",
    "            query = item['query']\n",
    "            positive = item['positive']\n",
    "            negatives = item.get('negatives', [])\n",
    "            \n",
    "            if negatives:\n",
    "\n",
    "                val_examples.append(InputExample(texts=[query, positive, negatives[0]]))\n",
    "            else:\n",
    "                val_examples.append(InputExample(texts=[query, positive]))\n",
    "\n",
    "        triplet_evaluator = evaluation.TripletEvaluator.from_input_examples(\n",
    "            val_examples,\n",
    "            name=\"validation_loss\",\n",
    "            write_csv=True\n",
    "        )\n",
    "        \n",
    "        val_queries = {f\"q{i}\": item['query'] for i, item in enumerate(val_data)}\n",
    "        val_corpus = {f\"d{i}\": item['positive'] for i, item in enumerate(val_data)}\n",
    "        val_relevant = {f\"q{i}\": {f\"d{i}\"} for i in range(len(val_data))}\n",
    "        \n",
    "        ir_evaluator = evaluation.InformationRetrievalEvaluator(\n",
    "            queries=val_queries,\n",
    "            corpus=val_corpus,\n",
    "            relevant_docs=val_relevant,\n",
    "            name=\"validation_metrics\",\n",
    "            write_csv=True\n",
    "        )\n",
    "        \n",
    "        evaluator = evaluation.SequentialEvaluator(\n",
    "            [triplet_evaluator, ir_evaluator],\n",
    "            main_score_function=lambda scores: scores[1] \n",
    "        )\n",
    "\n",
    "    print(f\"\\nTraining for {epochs} epochs...\")\n",
    "    warmup_steps = int(len(train_dataloader) * warmup_ratio)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=output_path,\n",
    "        show_progress_bar=True,\n",
    "        use_amp=use_amp,\n",
    "        evaluator=evaluator,\n",
    "        evaluation_steps=len(train_dataloader) if evaluator else 0,\n",
    "        save_best_model=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Training complete. Model saved to {output_path}\")\n",
    "\n",
    "    training_stats = {\n",
    "        'epochs': epochs,\n",
    "        'total_steps': len(train_dataloader) * epochs,\n",
    "        'warmup_steps': warmup_steps\n",
    "    }\n",
    "    \n",
    "    if val_data:\n",
    "        try:\n",
    "\n",
    "            loss_csv_path = os.path.join(output_path, \"validation_loss_evaluation_results.csv\")\n",
    "            if os.path.exists(loss_csv_path):\n",
    "                with open(loss_csv_path, 'r') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    loss_data = list(reader)\n",
    "                    training_stats['validation_loss_history'] = [\n",
    "                        float(row['cosine_accuracy']) for row in loss_data\n",
    "                    ]\n",
    "\n",
    "            metrics_csv_path = os.path.join(output_path, \"validation_metrics_evaluation_results.csv\")\n",
    "            if os.path.exists(metrics_csv_path):\n",
    "                with open(metrics_csv_path, 'r') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    metrics_data = list(reader)\n",
    "                    if metrics_data:\n",
    "                        last_metrics = metrics_data[-1]\n",
    "                        training_stats['final_metrics'] = {\n",
    "                            'accuracy@1': float(last_metrics.get('cosine_accuracy@1', 0)),\n",
    "                            'accuracy@10': float(last_metrics.get('cosine_accuracy@10', 0)),\n",
    "                            'ndcg@10': float(last_metrics.get('cosine_ndcg@10', 0)),\n",
    "                            'mrr@10': float(last_metrics.get('cosine_mrr@10', 0)),\n",
    "                            'map@100': float(last_metrics.get('cosine_map@100', 0))\n",
    "                        }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read validation metrics: {e}\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e29f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STEP 4: Splitting train/val/test...\n",
      "Train: 1421 | Val: 304 | Test: 305\n",
      "\n",
      "üèãÔ∏è STEP 5: Training custom embedding model...\n",
      "Loading base model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Preparing training data...\n",
      "Created 4263 training examples\n",
      "Using MultipleNegativesRankingLoss (with hard negatives)\n",
      "Setting up validation evaluators...\n",
      "\n",
      "Training for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [801/801 12:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Validation Loss Cosine Accuracy</th>\n",
       "      <th>Validation Metrics Cosine Accuracy@1</th>\n",
       "      <th>Validation Metrics Cosine Accuracy@3</th>\n",
       "      <th>Validation Metrics Cosine Accuracy@5</th>\n",
       "      <th>Validation Metrics Cosine Accuracy@10</th>\n",
       "      <th>Validation Metrics Cosine Precision@1</th>\n",
       "      <th>Validation Metrics Cosine Precision@3</th>\n",
       "      <th>Validation Metrics Cosine Precision@5</th>\n",
       "      <th>Validation Metrics Cosine Precision@10</th>\n",
       "      <th>Validation Metrics Cosine Recall@1</th>\n",
       "      <th>Validation Metrics Cosine Recall@3</th>\n",
       "      <th>Validation Metrics Cosine Recall@5</th>\n",
       "      <th>Validation Metrics Cosine Recall@10</th>\n",
       "      <th>Validation Metrics Cosine Ndcg@10</th>\n",
       "      <th>Validation Metrics Cosine Mrr@10</th>\n",
       "      <th>Validation Metrics Cosine Map@100</th>\n",
       "      <th>Sequential Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.891447</td>\n",
       "      <td>0.075658</td>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.075658</td>\n",
       "      <td>0.069079</td>\n",
       "      <td>0.055263</td>\n",
       "      <td>0.043092</td>\n",
       "      <td>0.075658</td>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.228736</td>\n",
       "      <td>0.167315</td>\n",
       "      <td>0.188066</td>\n",
       "      <td>0.228736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>1.696400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.203947</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.067982</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.045395</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.203947</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.249853</td>\n",
       "      <td>0.187439</td>\n",
       "      <td>0.208960</td>\n",
       "      <td>0.249853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>1.696400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.319079</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.069079</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.044079</td>\n",
       "      <td>0.095395</td>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.319079</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.244668</td>\n",
       "      <td>0.184523</td>\n",
       "      <td>0.207653</td>\n",
       "      <td>0.244668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training complete. Model saved to trained_model/custom_model\n"
     ]
    }
   ],
   "source": [
    "def Training_model(training_data_enriched: List[Dict]):\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "\n",
    "    CONFIG = {\n",
    "        'output_dir': OUTPUT_DIR,\n",
    "        'base_model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'epochs': 3,\n",
    "        'batch_size': 16,\n",
    "        'train_val_test_split': [0.7, 0.15, 0.15]\n",
    "    }\n",
    "\n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"\\n STEP 4: Splitting train/val/test...\")\n",
    "    random.shuffle(training_data_enriched)\n",
    "    \n",
    "    n = len(training_data_enriched)\n",
    "    train_size = int(CONFIG['train_val_test_split'][0] * n)\n",
    "    val_size = int(CONFIG['train_val_test_split'][1] * n)\n",
    "    \n",
    "    train_data = training_data_enriched[:train_size]\n",
    "    val_data = training_data_enriched[train_size:train_size+val_size]\n",
    "    test_data = training_data_enriched[train_size+val_size:]\n",
    "    \n",
    "    print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
    "\n",
    "    print(\"\\nüèãÔ∏è STEP 5: Training custom embedding model...\")\n",
    "    custom_model, training_stats = train_embedding_model(\n",
    "        train_data,\n",
    "        base_model_name=CONFIG['base_model'],\n",
    "        output_path=str(output_dir / \"custom_model\"),\n",
    "        val_data=val_data,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        batch_size=CONFIG['batch_size']\n",
    "    )\n",
    "\n",
    "    return training_stats\n",
    "\n",
    "training_stats = Training_model(training_data_enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941aedfd",
   "metadata": {},
   "source": [
    "### STEP 4: EVALUATION & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06aaca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(\n",
    "    model: SentenceTransformer,\n",
    "    test_data: List[Dict],\n",
    "    chunks: List[Dict],\n",
    "    k_values: List[int] = [1, 3, 5, 10]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive retrieval evaluation.\n",
    "    \n",
    "    Metrics:\n",
    "    - Recall@K: What fraction of queries retrieve the correct document in top-K\n",
    "    - MRR: Mean Reciprocal Rank\n",
    "    - MAP: Mean Average Precision\n",
    "    \"\"\"\n",
    "    corpus = [c['text'] for c in chunks]\n",
    "    chunk_id_to_idx = {c['chunk_id']: i for i, c in enumerate(chunks)}\n",
    "    \n",
    "    print(\"Encoding corpus for evaluation...\")\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "    \n",
    "    metrics = {f'recall@{k}': [] for k in k_values}\n",
    "    metrics['mrr'] = []\n",
    "    metrics['map'] = []\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        query = item['query']\n",
    "        target_id = item['chunk_id']\n",
    "        \n",
    "        target_idx = chunk_id_to_idx.get(target_id)\n",
    "        if target_idx is None:\n",
    "            continue\n",
    "\n",
    "        query_emb = model.encode(query, convert_to_tensor=True)\n",
    "        scores = cos_sim(query_emb, corpus_embeddings)[0]\n",
    "        top_indices = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "    \n",
    "        rank = np.where(top_indices == target_idx)[0][0] + 1\n",
    "\n",
    "        for k in k_values:\n",
    "            metrics[f'recall@{k}'].append(1.0 if rank <= k else 0.0)\n",
    "\n",
    "        metrics['mrr'].append(1.0 / rank)\n",
    "\n",
    "        metrics['map'].append(1.0 / rank)\n",
    "\n",
    "    results = {key: np.mean(values) for key, values in metrics.items()}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    base_model: SentenceTransformer,\n",
    "    custom_model: SentenceTransformer,\n",
    "    test_data: List[Dict],\n",
    "    chunks: List[Dict]\n",
    ") -> Dict:\n",
    "    \"\"\"Compare baseline vs custom model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATING BASELINE MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    base_results = evaluate_retrieval(base_model, test_data, chunks)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATING CUSTOM MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    custom_results = evaluate_retrieval(custom_model, test_data, chunks)\n",
    "    \n",
    "    return {'baseline': base_results, 'custom': custom_results}\n",
    "\n",
    "\n",
    "def plot_comparison(results: Dict, save_path: str = \"model_comparison.png\"):\n",
    "    \"\"\"Plot model comparison\"\"\"\n",
    "    metrics = list(results['baseline'].keys())\n",
    "    baseline_scores = [results['baseline'][m] for m in metrics]\n",
    "    custom_scores = [results['custom'][m] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, baseline_scores, width, label='Baseline', alpha=0.8)\n",
    "    ax.bar(x + width/2, custom_scores, width, label='Custom', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Baseline vs Custom Embedding Model', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úì Comparison plot saved: {save_path}\")\n",
    "\n",
    "\n",
    "def visualize_embeddings(\n",
    "    model: SentenceTransformer,\n",
    "    chunks: List[Dict],\n",
    "    sample_size: int = 500,\n",
    "    save_path: str = \"embedding_visualization.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize embeddings using t-SNE and compute clustering metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    sampled = random.sample(chunks, min(sample_size, len(chunks)))\n",
    "    texts = [c['text'] for c in sampled]\n",
    "    sources = [c['source'] for c in sampled]\n",
    "    \n",
    "    print(f\"Computing embeddings for {len(texts)} samples...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "    print(\"Computing t-SNE projection...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    unique_sources = list(set(sources))\n",
    "    source_labels = [unique_sources.index(s) for s in sources]\n",
    "    \n",
    "    if len(unique_sources) > 1:\n",
    "        silhouette = silhouette_score(embeddings, source_labels)\n",
    "        print(f\"‚úì Silhouette Score: {silhouette:.4f}\")\n",
    "    else:\n",
    "        silhouette = None\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_sources)))\n",
    "    \n",
    "    for i, source in enumerate(unique_sources):\n",
    "        mask = np.array(sources) == source\n",
    "        ax.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=[colors[i]],\n",
    "            label=source[:20],\n",
    "            alpha=0.6,\n",
    "            s=50\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    title = 'Embedding Space Visualization'\n",
    "    if silhouette:\n",
    "        title += f' (Silhouette: {silhouette:.3f})'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úì Visualization saved: {save_path}\")\n",
    "    \n",
    "    return {'silhouette_score': silhouette}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05941bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 6: Evaluating models...\n",
      "\n",
      "============================================================\n",
      "EVALUATING BASELINE MODEL\n",
      "============================================================\n",
      "Encoding corpus for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  5.16it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [00:02<00:00, 110.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING CUSTOM MODEL\n",
      "============================================================\n",
      "Encoding corpus for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  4.88it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 305/305 [00:02<00:00, 106.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "BASELINE MODEL:\n",
      "  recall@1    : 0.0492\n",
      "  recall@3    : 0.1082\n",
      "  recall@5    : 0.1344\n",
      "  recall@10   : 0.1869\n",
      "  mrr         : 0.1043\n",
      "  map         : 0.1043\n",
      "\n",
      "CUSTOM MODEL:\n",
      "  recall@1    : 0.0885\n",
      "  recall@3    : 0.1869\n",
      "  recall@5    : 0.2754\n",
      "  recall@10   : 0.4492\n",
      "  mrr         : 0.1929\n",
      "  map         : 0.1929\n",
      "\n",
      "üìà STEP 7: Creating visualizations...\n",
      "‚úì Comparison plot saved: train_data/model_comparison.png\n",
      "\n",
      "Visualizing baseline embeddings...\n",
      "Computing embeddings for 407 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE projection...\n",
      "‚úì Silhouette Score: 0.0452\n",
      "‚úì Visualization saved: train_data/baseline_embeddings.png\n",
      "\n",
      "Visualizing custom embeddings...\n",
      "Computing embeddings for 407 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE projection...\n",
      "‚úì Silhouette Score: 0.0574\n",
      "‚úì Visualization saved: train_data/custom_embeddings.png\n",
      "\n",
      "============================================================\n",
      "SUMMARY REPORT\n",
      "============================================================\n",
      "\n",
      "Total chunks processed: 407\n",
      "Training examples generated: 2030\n",
      "\n",
      "Clustering Quality:\n",
      "  Baseline Silhouette: 0.045152705162763596\n",
      "  Custom Silhouette: 0.05740627273917198\n",
      "\n",
      "‚úÖ All outputs saved to: train_data\n",
      "‚úÖ Custom model saved to: train_data/custom_model\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        'pdf_folder': DATA_DIR,\n",
    "        'output_dir': TRAINING_DATA_DIR,\n",
    "        'base_model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'epochs': 3,\n",
    "        'batch_size': 16,\n",
    "        'queries_per_chunk': 5,\n",
    "        'num_hard_negatives': 3,\n",
    "        'train_val_test_split': [0.7, 0.15, 0.15]\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(CONFIG['output_dir'])\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    base_model = SentenceTransformer(CONFIG['base_model'])\n",
    "\n",
    "    custom_model = SentenceTransformer(\"./trained_model/custom_model/\")\n",
    "    n = len(training_data_enriched)\n",
    "    train_size = int(CONFIG['train_val_test_split'][0] * n)\n",
    "    val_size = int(CONFIG['train_val_test_split'][1] * n)\n",
    "    \n",
    "    test_data = training_data_enriched[train_size+val_size:]\n",
    "    # Step 6: Evaluate\n",
    "    print(\"\\nüìä STEP 6: Evaluating models...\")\n",
    "    results = compare_models(base_model, custom_model, test_data, chunks)\n",
    "    \n",
    "    # Save results\n",
    "    with open(output_dir / \"evaluation_results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for model_name in ['baseline', 'custom']:\n",
    "        print(f\"\\n{model_name.upper()} MODEL:\")\n",
    "        for metric, value in results[model_name].items():\n",
    "            print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    \n",
    "    # Step 7: Visualizations\n",
    "    print(\"\\nüìà STEP 7: Creating visualizations...\")\n",
    "    \n",
    "    # Model comparison\n",
    "    plot_comparison(results, str(output_dir / \"model_comparison.png\"))\n",
    "    \n",
    "    # Baseline embeddings\n",
    "    print(\"\\nVisualizing baseline embeddings...\")\n",
    "    baseline_metrics = visualize_embeddings(\n",
    "        base_model, \n",
    "        chunks, \n",
    "        save_path=str(output_dir / \"baseline_embeddings.png\")\n",
    "    )\n",
    "    \n",
    "    # Custom embeddings\n",
    "    print(\"\\nVisualizing custom embeddings...\")\n",
    "    custom_metrics = visualize_embeddings(\n",
    "        custom_model, \n",
    "        chunks, \n",
    "        save_path=str(output_dir / \"custom_embeddings.png\")\n",
    "    )\n",
    "    \n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal chunks processed: {len(chunks)}\")\n",
    "    print(f\"Training examples generated: {len(training_data_enriched)}\")\n",
    "    print(f\"\\nClustering Quality:\")\n",
    "    print(f\"  Baseline Silhouette: {baseline_metrics.get('silhouette_score', 'N/A')}\")\n",
    "    print(f\"  Custom Silhouette: {custom_metrics.get('silhouette_score', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All outputs saved to: {output_dir}\")\n",
    "    print(f\"‚úÖ Custom model saved to: {output_dir / 'custom_model'}\")\n",
    "\n",
    "evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "001f505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLUSTERING & VISUALIZATION ANALYSIS\n",
      "======================================================================\n",
      "üì• Loading data from ./train_data/chunks.json...\n",
      "   ‚úì Loaded 407 chunks from 5 sources\n",
      "\n",
      "üî¢ Generating embeddings...\n",
      "   Baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:02<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Custom model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:01<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Computing clustering metrics (k=5)...\n",
      "\n",
      "   Baseline Metrics:\n",
      "      silhouette: 0.0658\n",
      "      calinski_harabasz: 17.5987\n",
      "      davies_bouldin: 3.3111\n",
      "      inertia: 247.6582\n",
      "\n",
      "   Custom Metrics:\n",
      "      silhouette: 0.0608\n",
      "      calinski_harabasz: 17.3790\n",
      "      davies_bouldin: 3.5625\n",
      "      inertia: 326.2460\n",
      "\n",
      "üìè Computing cluster separation...\n",
      "\n",
      "   Baseline - Separation Ratio: 0.7627\n",
      "   Custom - Separation Ratio: 0.6934\n",
      "\n",
      "üéØ Checking neighbor coherence...\n",
      "\n",
      "   Baseline - Avg Coherence: 0.6157\n",
      "   Custom - Avg Coherence: 0.6531\n",
      "\n",
      "üìà Computing similarity distributions...\n",
      "\n",
      "üé® Generating visualizations...\n",
      "‚úì Saved: ./results/embeddings_umap.png\n",
      "‚úì Saved: ./results/metrics_comparison.png\n",
      "‚úì Saved: ./results/similarity_distribution.png\n",
      "‚úì Saved: ./results/elbow_plot.png\n",
      "‚úì Saved: ./results/neighbor_coherence.png\n",
      "\n",
      "======================================================================\n",
      "SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "üìà Custom Model Improvements:\n",
      "   ‚úó Silhouette Score: -7.63%\n",
      "   ‚úó Calinski-Harabasz: -1.25%\n",
      "   ‚úó Davies-Bouldin (lower is better): -7.59%\n",
      "   ‚úó Separation Ratio: -9.09%\n",
      "   ‚úì Neighbor Coherence: +6.07%\n",
      "\n",
      "‚úÖ Analysis complete! All plots saved.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clustering Analysis & Visualization for Custom vs Baseline Embeddings\n",
    "UPDATED: Loads chunks from extracted_chunks.json\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS & DATA (UPDATED)\n",
    "# ============================================================================\n",
    "\n",
    "def load_models_and_data(base_model_path: str, custom_model_path: str, \n",
    "                         chunks_file: str ):\n",
    "    \"\"\"Load both models and chunks from JSON\"\"\"\n",
    "    print(f\"üì• Loading data from {chunks_file}...\")\n",
    "    \n",
    "    base_model = SentenceTransformer(base_model_path)\n",
    "    custom_model = SentenceTransformer(custom_model_path)\n",
    "    \n",
    "    # Load chunks from JSON\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    texts = [c['text'][:300] for c in chunks]  # Truncate for speed\n",
    "    sources = [c.get('source', 'unknown') for c in chunks]\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(texts)} chunks from {len(set(sources))} sources\")\n",
    "    \n",
    "    return base_model, custom_model, texts, sources, chunks\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EMBEDDING GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_embeddings(model: SentenceTransformer, texts: list) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings with progress bar\"\"\"\n",
    "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC 1: CLUSTERING QUALITY METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_clustering_metrics(embeddings: np.ndarray, n_clusters: int = 5):\n",
    "    \"\"\"Compute Silhouette, Calinski-Harabasz, Davies-Bouldin scores\"\"\"\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    metrics = {\n",
    "        'silhouette': silhouette_score(embeddings, labels),\n",
    "        'calinski_harabasz': calinski_harabasz_score(embeddings, labels),\n",
    "        'davies_bouldin': davies_bouldin_score(embeddings, labels),\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC 2: INTRA-CLUSTER vs INTER-CLUSTER DISTANCE\n",
    "# ============================================================================\n",
    "\n",
    "def compute_cluster_separation(embeddings: np.ndarray, labels: np.ndarray):\n",
    "    \"\"\"Calculate average intra-cluster vs inter-cluster distances\"\"\"\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_centers = []\n",
    "    \n",
    "    # Compute cluster centers\n",
    "    for label in unique_labels:\n",
    "        cluster_points = embeddings[labels == label]\n",
    "        cluster_centers.append(cluster_points.mean(axis=0))\n",
    "    \n",
    "    cluster_centers = np.array(cluster_centers)\n",
    "    \n",
    "    # Intra-cluster distance (average distance within clusters)\n",
    "    intra_distances = []\n",
    "    for label in unique_labels:\n",
    "        cluster_points = embeddings[labels == label]\n",
    "        center = cluster_centers[label]\n",
    "        distances = np.linalg.norm(cluster_points - center, axis=1)\n",
    "        intra_distances.extend(distances)\n",
    "    \n",
    "    # Inter-cluster distance (distance between cluster centers)\n",
    "    inter_distances = []\n",
    "    for i in range(len(cluster_centers)):\n",
    "        for j in range(i+1, len(cluster_centers)):\n",
    "            dist = np.linalg.norm(cluster_centers[i] - cluster_centers[j])\n",
    "            inter_distances.append(dist)\n",
    "    \n",
    "    return {\n",
    "        'avg_intra_distance': np.mean(intra_distances),\n",
    "        'avg_inter_distance': np.mean(inter_distances),\n",
    "        'separation_ratio': np.mean(inter_distances) / np.mean(intra_distances)\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC 3: NEIGHBOR COHERENCE CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def neighbor_coherence_check(embeddings: np.ndarray, sources: list, k: int = 10):\n",
    "    \"\"\"Check if k-nearest neighbors come from the same source document\"\"\"\n",
    "    \n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    \n",
    "    coherence_scores = []\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        neighbor_indices = indices[i][1:]  # Exclude self\n",
    "        neighbor_sources = [sources[idx] for idx in neighbor_indices]\n",
    "        \n",
    "        # Calculate percentage of neighbors from same source\n",
    "        same_source_count = sum(1 for s in neighbor_sources if s == sources[i])\n",
    "        coherence_scores.append(same_source_count / k)\n",
    "    \n",
    "    return {\n",
    "        'avg_coherence': np.mean(coherence_scores),\n",
    "        'median_coherence': np.median(coherence_scores),\n",
    "        'std_coherence': np.std(coherence_scores)\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRIC 4: PAIRWISE SIMILARITY DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "def pairwise_similarity_distribution(embeddings: np.ndarray, sample_size: int = 1000):\n",
    "    \"\"\"Compute distribution of pairwise cosine similarities\"\"\"\n",
    "    \n",
    "    # Sample for efficiency\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        sampled_embeddings = embeddings[indices]\n",
    "    else:\n",
    "        sampled_embeddings = embeddings\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(sampled_embeddings)\n",
    "    \n",
    "    # Get upper triangle (exclude diagonal)\n",
    "    triu_indices = np.triu_indices_from(similarities, k=1)\n",
    "    similarity_values = similarities[triu_indices]\n",
    "    \n",
    "    return {\n",
    "        'mean_similarity': np.mean(similarity_values),\n",
    "        'std_similarity': np.std(similarity_values),\n",
    "        'min_similarity': np.min(similarity_values),\n",
    "        'max_similarity': np.max(similarity_values),\n",
    "        'similarities': similarity_values\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 1: UMAP/t-SNE 2D PROJECTION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_embeddings_2d(base_embeddings: np.ndarray, custom_embeddings: np.ndarray, \n",
    "                       labels_base: np.ndarray, labels_custom: np.ndarray,\n",
    "                       method: str = 'umap', save_path: str = './results/embeddings_2d.png'):\n",
    "    \"\"\"Plot 2D projections of embeddings\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "\n",
    "    base_2d = reducer.fit_transform(base_embeddings)\n",
    "    axes[0].scatter(base_2d[:, 0], base_2d[:, 1], c=labels_base, cmap='tab10', alpha=0.6, s=20)\n",
    "    axes[0].set_title(f'Baseline Model - {method.upper()}', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Component 1')\n",
    "    axes[0].set_ylabel('Component 2')\n",
    "    \n",
    "    # Custom model\n",
    "    reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    \n",
    "    custom_2d = reducer.fit_transform(custom_embeddings)\n",
    "    axes[1].scatter(custom_2d[:, 0], custom_2d[:, 1], c=labels_custom, cmap='tab10', alpha=0.6, s=20)\n",
    "    axes[1].set_title(f'Custom Model - {method.upper()}', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Component 1')\n",
    "    axes[1].set_ylabel('Component 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved: {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 2: CLUSTERING METRICS COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def plot_metrics_comparison(base_metrics: dict, custom_metrics: dict, \n",
    "                            save_path: str = './results/metrics_comparison.png'):\n",
    "    \"\"\"Bar plot comparing clustering metrics\"\"\"\n",
    "    \n",
    "    metrics_to_plot = ['silhouette', 'calinski_harabasz', 'davies_bouldin']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        base_val = base_metrics[metric]\n",
    "        custom_val = custom_metrics[metric]\n",
    "        \n",
    "        # For Davies-Bouldin, lower is better\n",
    "        if metric == 'davies_bouldin':\n",
    "            better = 'Custom' if custom_val < base_val else 'Base'\n",
    "            colors = ['#d62728', '#2ca02c'] if custom_val < base_val else ['#2ca02c', '#d62728']\n",
    "        else:\n",
    "            better = 'Custom' if custom_val > base_val else 'Base'\n",
    "            colors = ['#d62728', '#2ca02c'] if custom_val > base_val else ['#2ca02c', '#d62728']\n",
    "        \n",
    "        axes[idx].bar(['Baseline', 'Custom'], [base_val, custom_val], color=colors, alpha=0.7)\n",
    "        axes[idx].set_title(metric.replace('_', ' ').title(), fontweight='bold')\n",
    "        axes[idx].set_ylabel('Score')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add improvement text\n",
    "        improvement = ((custom_val - base_val) / base_val) * 100\n",
    "        axes[idx].text(0.5, max(base_val, custom_val) * 1.05, \n",
    "                      f'{improvement:+.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved: {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 3: SIMILARITY DISTRIBUTION HISTOGRAM\n",
    "# ============================================================================\n",
    "\n",
    "def plot_similarity_distribution(base_sims: np.ndarray, custom_sims: np.ndarray,\n",
    "                                 save_path: str = './results/similarity_distribution.png'):\n",
    "    \"\"\"Histogram of pairwise similarity distributions\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Base model\n",
    "    axes[0].hist(base_sims, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[0].axvline(base_sims.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {base_sims.mean():.3f}')\n",
    "    axes[0].set_title('Baseline Model - Similarity Distribution', fontweight='bold')\n",
    "    axes[0].set_xlabel('Cosine Similarity')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Custom model\n",
    "    axes[1].hist(custom_sims, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "    axes[1].axvline(custom_sims.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {custom_sims.mean():.3f}')\n",
    "    axes[1].set_title('Custom Model - Similarity Distribution', fontweight='bold')\n",
    "    axes[1].set_xlabel('Cosine Similarity')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved: {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 4: ELBOW PLOT (Optimal K)\n",
    "# ============================================================================\n",
    "\n",
    "def plot_elbow_curve(base_embeddings: np.ndarray, custom_embeddings: np.ndarray,\n",
    "                     k_range: range = range(2, 11), save_path: str = './results/elbow_plot.png'):\n",
    "    \"\"\"Elbow method to find optimal number of clusters\"\"\"\n",
    "    \n",
    "    base_inertias = []\n",
    "    custom_inertias = []\n",
    "    base_silhouettes = []\n",
    "    custom_silhouettes = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Base model\n",
    "        kmeans_base = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels_base = kmeans_base.fit_predict(base_embeddings)\n",
    "        base_inertias.append(kmeans_base.inertia_)\n",
    "        base_silhouettes.append(silhouette_score(base_embeddings, labels_base))\n",
    "        \n",
    "        # Custom model\n",
    "        kmeans_custom = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels_custom = kmeans_custom.fit_predict(custom_embeddings)\n",
    "        custom_inertias.append(kmeans_custom.inertia_)\n",
    "        custom_silhouettes.append(silhouette_score(custom_embeddings, labels_custom))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Inertia plot\n",
    "    axes[0].plot(k_range, base_inertias, marker='o', label='Baseline', linewidth=2)\n",
    "    axes[0].plot(k_range, custom_inertias, marker='s', label='Custom', linewidth=2)\n",
    "    axes[0].set_title('Elbow Plot - Inertia', fontweight='bold')\n",
    "    axes[0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0].set_ylabel('Inertia')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Silhouette plot\n",
    "    axes[1].plot(k_range, base_silhouettes, marker='o', label='Baseline', linewidth=2)\n",
    "    axes[1].plot(k_range, custom_silhouettes, marker='s', label='Custom', linewidth=2)\n",
    "    axes[1].set_title('Silhouette Score vs K', fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved: {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION 5: NEIGHBOR COHERENCE HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "def plot_neighbor_coherence(base_embeddings: np.ndarray, custom_embeddings: np.ndarray,\n",
    "                            sources: list, k_values: list = [5, 10, 20],\n",
    "                            save_path: str = './results/neighbor_coherence.png'):\n",
    "    \"\"\"Plot neighbor coherence for different k values\"\"\"\n",
    "    \n",
    "    base_coherences = []\n",
    "    custom_coherences = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        base_result = neighbor_coherence_check(base_embeddings, sources, k)\n",
    "        custom_result = neighbor_coherence_check(custom_embeddings, sources, k)\n",
    "        base_coherences.append(base_result['avg_coherence'])\n",
    "        custom_coherences.append(custom_result['avg_coherence'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(k_values))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, base_coherences, width, label='Baseline', alpha=0.8, color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, custom_coherences, width, label='Custom', alpha=0.8, color='coral')\n",
    "    \n",
    "    ax.set_xlabel('k (Number of Neighbors)', fontweight='bold')\n",
    "    ax.set_ylabel('Neighbor Coherence Score', fontweight='bold')\n",
    "    ax.set_title('Same-Source Neighbor Coherence', fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'k={k}' for k in k_values])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved: {save_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS PIPELINE (UPDATED)\n",
    "# ============================================================================\n",
    "\n",
    "def run_complete_analysis(base_model_path: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                         custom_model_path: str = \"./custom_embedding_model\", \n",
    "                         chunks_file: str = \"extracted_chunks.json\",\n",
    "                         n_clusters: int = 5):\n",
    "    \"\"\"Run complete clustering and visualization analysis\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CLUSTERING & VISUALIZATION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    base_model, custom_model, texts, sources, chunks = load_models_and_data(\n",
    "        base_model_path, custom_model_path, chunks_file\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"\\nüî¢ Generating embeddings...\")\n",
    "    print(\"   Baseline model...\")\n",
    "    base_embeddings = generate_embeddings(base_model, texts)\n",
    "    print(\"   Custom model...\")\n",
    "    custom_embeddings = generate_embeddings(custom_model, texts)\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(f\"\\nüìä Computing clustering metrics (k={n_clusters})...\")\n",
    "    base_metrics = compute_clustering_metrics(base_embeddings, n_clusters)\n",
    "    custom_metrics = compute_clustering_metrics(custom_embeddings, n_clusters)\n",
    "    \n",
    "    print(\"\\n   Baseline Metrics:\")\n",
    "    for k, v in base_metrics.items():\n",
    "        if k != 'labels':\n",
    "            print(f\"      {k}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\n   Custom Metrics:\")\n",
    "    for k, v in custom_metrics.items():\n",
    "        if k != 'labels':\n",
    "            print(f\"      {k}: {v:.4f}\")\n",
    "    \n",
    "    # Cluster separation\n",
    "    print(\"\\nüìè Computing cluster separation...\")\n",
    "    base_separation = compute_cluster_separation(base_embeddings, base_metrics['labels'])\n",
    "    custom_separation = compute_cluster_separation(custom_embeddings, custom_metrics['labels'])\n",
    "    \n",
    "    print(f\"\\n   Baseline - Separation Ratio: {base_separation['separation_ratio']:.4f}\")\n",
    "    print(f\"   Custom - Separation Ratio: {custom_separation['separation_ratio']:.4f}\")\n",
    "    \n",
    "    # Neighbor coherence\n",
    "    print(\"\\nüéØ Checking neighbor coherence...\")\n",
    "    base_coherence = neighbor_coherence_check(base_embeddings, sources, k=10)\n",
    "    custom_coherence = neighbor_coherence_check(custom_embeddings, sources, k=10)\n",
    "    \n",
    "    print(f\"\\n   Baseline - Avg Coherence: {base_coherence['avg_coherence']:.4f}\")\n",
    "    print(f\"   Custom - Avg Coherence: {custom_coherence['avg_coherence']:.4f}\")\n",
    "    \n",
    "    # Similarity distribution\n",
    "    print(\"\\nüìà Computing similarity distributions...\")\n",
    "    base_sim_dist = pairwise_similarity_distribution(base_embeddings)\n",
    "    custom_sim_dist = pairwise_similarity_distribution(custom_embeddings)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\nüé® Generating visualizations...\")\n",
    "    \n",
    "    plot_embeddings_2d(base_embeddings, custom_embeddings, \n",
    "                      base_metrics['labels'], custom_metrics['labels'],\n",
    "                      method='umap', save_path='./results/embeddings_umap.png')\n",
    "    \n",
    "    plot_metrics_comparison(base_metrics, custom_metrics, \n",
    "                           save_path='./results/metrics_comparison.png')\n",
    "    \n",
    "    plot_similarity_distribution(base_sim_dist['similarities'], \n",
    "                                custom_sim_dist['similarities'],\n",
    "                                save_path='./results/similarity_distribution.png')\n",
    "    \n",
    "    plot_elbow_curve(base_embeddings, custom_embeddings,\n",
    "                    save_path='./results/elbow_plot.png')\n",
    "    \n",
    "    plot_neighbor_coherence(base_embeddings, custom_embeddings, sources,\n",
    "                           save_path='./results/neighbor_coherence.png')\n",
    "    \n",
    "    # Summary report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    improvements = {\n",
    "        'Silhouette Score': ((custom_metrics['silhouette'] - base_metrics['silhouette']) / base_metrics['silhouette'] * 100),\n",
    "        'Calinski-Harabasz': ((custom_metrics['calinski_harabasz'] - base_metrics['calinski_harabasz']) / base_metrics['calinski_harabasz'] * 100),\n",
    "        'Davies-Bouldin (lower is better)': ((base_metrics['davies_bouldin'] - custom_metrics['davies_bouldin']) / base_metrics['davies_bouldin'] * 100),\n",
    "        'Separation Ratio': ((custom_separation['separation_ratio'] - base_separation['separation_ratio']) / base_separation['separation_ratio'] * 100),\n",
    "        'Neighbor Coherence': ((custom_coherence['avg_coherence'] - base_coherence['avg_coherence']) / base_coherence['avg_coherence'] * 100)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìà Custom Model Improvements:\")\n",
    "    for metric, improvement in improvements.items():\n",
    "        symbol = \"‚úì\" if improvement > 0 else \"‚úó\"\n",
    "        print(f\"   {symbol} {metric}: {improvement:+.2f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Analysis complete! All plots saved.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Return metrics for report generation\n",
    "    return {\n",
    "        'base_metrics': base_metrics,\n",
    "        'custom_metrics': custom_metrics,\n",
    "        'base_separation': base_separation,\n",
    "        'custom_separation': custom_separation,\n",
    "        'base_coherence': base_coherence,\n",
    "        'custom_coherence': custom_coherence\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_complete_analysis(\n",
    "        base_model_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        custom_model_path=\"./trained_model/custom_model/\",\n",
    "        chunks_file=\"./train_data/chunks.json\",  # YOUR FILE\n",
    "        n_clusters=5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
